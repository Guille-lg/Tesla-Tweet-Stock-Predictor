{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer, sample_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los datasets de entrenamiento y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Link       50 non-null     object\n",
      " 1   Date       50 non-null     object\n",
      " 2   Text       50 non-null     object\n",
      " 3   Sentiment  50 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/SallyFereday/status/159006...</td>\n",
       "      <td>11/8/2022 23:10</td>\n",
       "      <td>Today TSLA is selling at a week low Some may ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/MyBostonRealtor/status/151...</td>\n",
       "      <td>4/20/2022 6:18</td>\n",
       "      <td>teslamobile TmobileTeslaTuesdays contest just ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/Ev92Revolution/status/1544...</td>\n",
       "      <td>7/5/2022 14:17</td>\n",
       "      <td>thats funny You praise VW  because its a germa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/LinnieL7/status/1532796385...</td>\n",
       "      <td>6/3/2022 23:18</td>\n",
       "      <td>Well see if the smartest man on the planet can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/TeslaradarB/status/1534895...</td>\n",
       "      <td>6/9/2022 18:19</td>\n",
       "      <td>Smooth EvaTheKfer nailed it by spotting a Tesl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link             Date  \\\n",
       "0  https://twitter.com/SallyFereday/status/159006...  11/8/2022 23:10   \n",
       "1  https://twitter.com/MyBostonRealtor/status/151...   4/20/2022 6:18   \n",
       "2  https://twitter.com/Ev92Revolution/status/1544...   7/5/2022 14:17   \n",
       "3  https://twitter.com/LinnieL7/status/1532796385...   6/3/2022 23:18   \n",
       "4  https://twitter.com/TeslaradarB/status/1534895...   6/9/2022 18:19   \n",
       "\n",
       "                                                Text  Sentiment  \n",
       "0   Today TSLA is selling at a week low Some may ...          0  \n",
       "1  teslamobile TmobileTeslaTuesdays contest just ...          1  \n",
       "2  thats funny You praise VW  because its a germa...          1  \n",
       "3  Well see if the smartest man on the planet can...          0  \n",
       "4  Smooth EvaTheKfer nailed it by spotting a Tesl...          1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/clean/tesla_tweets_clean_random_labelled.csv')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       50 non-null     object\n",
      " 1   Sentiment  50 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 928.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Link', 'Date'], axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['Sentiment'] == 1\n",
    "\n",
    "aux1 = df[mask].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "aux2 = df[~mask].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df_true, test_df_true = train_test_split(aux1, test_size=0.2, random_state=42)\n",
    "train_df_false, test_df_false = train_test_split(aux2, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.concat([train_df_true, train_df_false]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat([test_df_true, test_df_false]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df.rename(columns={'Sentiment': 'label', 'Text': 'text'}, inplace=True)\n",
    "test_df.rename(columns={'Sentiment': 'label', 'Text': 'text'}, inplace=True)\n",
    "\n",
    "train_df.to_csv('../../data/clean/tesla_tweets_clean_random_labelled_train.csv', index=False)\n",
    "test_df.to_csv('../../data/clean/tesla_tweets_clean_random_labelled_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to c:/Users/34644/Desktop/Cursos/Curso CEI/Trabajo final/data/cache/csv/default-f1a0eb82e5152487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 997.69it/s]\n",
      "\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 83.19it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to c:/Users/34644/Desktop/Cursos/Curso CEI/Trabajo final/data/cache/csv/default-f1a0eb82e5152487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 39\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train': '../../data/clean/tesla_tweets_clean_random_labelled_train.csv',\n",
    "        'test': '../../data/clean/tesla_tweets_clean_random_labelled_test.csv'\n",
    "    },\n",
    "    cache_dir='../../data/cache'\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 566kB/s]\n",
      "\n",
      "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<?, ?B/s]\n",
      "\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 10.4MB/s]\n",
      "\n",
      "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "\n",
      "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 525kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [05:33<00:00, 1.31MB/s]\n",
      "\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 239kB/s]\n",
      "\n",
      "\u001b[A\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.02MB/s]\n",
      "\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 356kB/s]\n",
      "\n",
      "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 13.1MB/s]\n",
      "\n",
      "\u001b[A\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 763kB/s]\n",
      "\n",
      "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=18, \n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Generating Training Pairs: 100%|██████████| 18/18 [00:00<00:00, 74.07it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 1404\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 88\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Iteration: 100%|██████████| 88/88 [23:18<00:00, 15.89s/it]\n",
      "\n",
      "Epoch: 100%|██████████| 1/1 [23:18<00:00, 1398.34s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9090909090909091}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(save_directory='./output_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model([\n",
    "    'Tesla stock is so down today',\n",
    "    'Tesla stock is so up today'\n",
    "    'I love tesla so much',\n",
    "    'I hate tesla so much',\n",
    "    'i crashed my tesla today',\n",
    "    'elon musk just saved a cat from a fire',\n",
    "    'i feel like elon musk is not doing enough for the environment',\n",
    "])\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
